{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import randint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def scrape_keyword_tribuna_hoje(keyword, max_page,save_path):\n",
    "\n",
    "  # Creates lists to store scraped data at each loop iteration\n",
    "  titles        = []\n",
    "  links         = []\n",
    "  dates_authors = []\n",
    "\n",
    "  scraped_data_dict = {\n",
    "                      'Titles'        : [],\n",
    "                      'Dates_Authors' : [],\n",
    "                      'Links'         : []\n",
    "                      }\n",
    "\n",
    "  # Replaces spaces by website's URL standard notation in keyword parameter\n",
    "  keyword_no_space = keyword.replace(' ','+')\n",
    "\n",
    "  # Starts scraping loop fo given page range\n",
    "  for page in range(1, max_page):\n",
    "\n",
    "    # Creates random delays to avoid blocking\n",
    "    delay = randint(0,3)\n",
    "    print(f'Cool down of {delay}.\\nScraping page number {page}')\n",
    "    sleep(delay)\n",
    "\n",
    "    # Defines each search query URL, stores request content and creates html_doc variable for scraping\n",
    "    url      = f'https://tribunahoje.com/resultado-da-busca/1682311238/{keyword_no_space}/pagina-{page}'\n",
    "    response = requests.get(url)\n",
    "    html_doc = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Tests request result. Stops loop if any error is given.\n",
    "    if str(response) != '<Response [200]>':\n",
    "      print(url); print(f\"Request error!\\n{response}\\nLoop stopped.\")\n",
    "      break\n",
    "\n",
    "    # Starts scraping if no error is given\n",
    "    else:\n",
    "      # Defines html pattern present only when results are found and defines it as stop sign to trigger loop break\n",
    "      stop_sign = html_doc.find('a', class_ = \"news-card-intern\")\n",
    "      if stop_sign == None:\n",
    "        loop = 'stop'\n",
    "      else :\n",
    "        loop = 'go'\n",
    "\n",
    "      # Breaks loop if stop sign is found\n",
    "      if loop == 'stop':\n",
    "        print(f'Found stop_sign: {stop_sign}')\n",
    "        print('Stop trigger activated.')\n",
    "        print(f'Stopped at search page {page}')\n",
    "        break\n",
    "\n",
    "      # Scrapes search result page if no stop sign is found\n",
    "      if loop == 'go':\n",
    "        cards = html_doc.find_all('a', class_ = \"news-card-intern\")\n",
    "        for elem in cards:\n",
    "          # Scrapes articles titles\n",
    "          title = elem.find('h1')\n",
    "          titles.append(title.text)\n",
    "\n",
    "          # Scrapes articles dates and authors\n",
    "          date_author = elem.find('span')\n",
    "          dates_authors.append(date_author.text)\n",
    "\n",
    "          # Scrapes articles links\n",
    "          link = elem['href']\n",
    "          links.append(link)\n",
    "\n",
    "  # Checks lists lengths to ensure syncing\n",
    "  a = len(titles)\n",
    "  b = len(dates_authors)\n",
    "  c = len(links)\n",
    "  print(f'Length of scraped titles list = {a}')\n",
    "  print(f'Length of scraped dates and authors list = {b}')\n",
    "  print(f'Length of scraped links list = {c}')\n",
    "  if a == b and a == c:\n",
    "    print(f'Scraped data lists with same lengths: {a} items.\\nCreating Data Frame.')\n",
    "    # Updates scraped data dict with lists and creates pandas df\n",
    "    scraped_data_dict['Titles']        = titles\n",
    "    scraped_data_dict['Dates_Authors'] = dates_authors\n",
    "    scraped_data_dict['Links']         = links\n",
    "    scraped_data_df                    = pd.DataFrame.from_dict(scraped_data_dict)\n",
    "    scraped_data_df.to_excel(f'{save_path}\\scraping_tribuna_hoje-{keyword}.xlsx', index = False )\n",
    "    return scraped_data_df\n",
    "  else :\n",
    "    print('Error!\\nLists with different lengths.\\nNo Data Frame created.')\n",
    "    return None\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'C:\\\\Users\\\\Jo√£o Flavio\\\\Desktop\\\\Code\\\\Scraping Jornais Alagoas\\\\tests'\n",
    "teste = scrape_keyword_tribuna_hoje('alagamento', 10, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
